{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/neural-machine-translation-with-python-c2f0a34f7dd\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev = '.csv'\n",
    "clean = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    num_words=3000\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=\"<UKN>\")\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data(abbrev, clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev_lst = list(data.abbrev_df['Original'])\n",
    "clean_lst = list(data.clean_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max Abbreviation length: 5\n",
      "Max Clean length: 5\n",
      "Abbreviation vocabulary size: 3103\n",
      "Clean vocabulary size: 1162\n"
     ]
    }
   ],
   "source": [
    "preproc_abbrev, preproc_clean, abbrev_tokenizer, clean_tokenizer = preprocess(abbrev_lst, clean_lst)\n",
    "    \n",
    "max_abbrev_length = preproc_abbrev.shape[1]\n",
    "max_clean_length = preproc_clean.shape[1]\n",
    "abbrev_vocab_size = len(abbrev_tokenizer.word_index)\n",
    "clean_vocab_size = len(clean_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max Abbreviation length:\", max_abbrev_length)\n",
    "print(\"Max Clean length:\", max_clean_length)\n",
    "print(\"Abbreviation vocabulary size:\", abbrev_vocab_size)\n",
    "print(\"Clean vocabulary size:\", clean_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(preds_index,preds,prob_threshold,tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for x in preds:\n",
    "        if x < prob_threshold:\n",
    "            preds_index[counter] = 1.0\n",
    "        else:\n",
    "            x\n",
    "        counter += 1\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in preds_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "#     model.add(GRU(256, return_sequences=True))   \n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = pad(preproc_abbrev, preproc_clean.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_clean.shape[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 256)            794624    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 5, 512)            787968    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 5, 1024)           525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 1024)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 5, 1163)           1192075   \n",
      "=================================================================\n",
      "Total params: 3,299,979\n",
      "Trainable params: 3,299,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_clean,\n",
    "    len(abbrev_tokenizer.word_index)+1,\n",
    "    len(clean_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.monger/anaconda3/envs/name_quality/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3389 samples, validate on 377 samples\n",
      "Epoch 1/1000\n",
      "3389/3389 [==============================] - 6s 2ms/step - loss: 3.0588 - accuracy: 0.5799 - val_loss: 2.6779 - val_accuracy: 0.5809\n",
      "Epoch 2/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 2.3118 - accuracy: 0.6239 - val_loss: 2.0332 - val_accuracy: 0.6520\n",
      "Epoch 3/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 1.7220 - accuracy: 0.6945 - val_loss: 1.6018 - val_accuracy: 0.7247\n",
      "Epoch 4/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 1.2440 - accuracy: 0.7721 - val_loss: 1.1959 - val_accuracy: 0.8064\n",
      "Epoch 5/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.8700 - accuracy: 0.8383 - val_loss: 0.9481 - val_accuracy: 0.8520\n",
      "Epoch 6/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.6332 - accuracy: 0.8810 - val_loss: 0.7720 - val_accuracy: 0.8737\n",
      "Epoch 7/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.4647 - accuracy: 0.9081 - val_loss: 0.6754 - val_accuracy: 0.8918\n",
      "Epoch 8/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.3482 - accuracy: 0.9291 - val_loss: 0.5934 - val_accuracy: 0.9066\n",
      "Epoch 9/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.2702 - accuracy: 0.9427 - val_loss: 0.5246 - val_accuracy: 0.9146\n",
      "Epoch 10/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.2113 - accuracy: 0.9536 - val_loss: 0.4739 - val_accuracy: 0.9279\n",
      "Epoch 11/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1747 - accuracy: 0.9628 - val_loss: 0.4593 - val_accuracy: 0.9342\n",
      "Epoch 12/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1332 - accuracy: 0.9699 - val_loss: 0.4226 - val_accuracy: 0.9332\n",
      "Epoch 13/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1170 - accuracy: 0.9740 - val_loss: 0.3955 - val_accuracy: 0.9422\n",
      "Epoch 14/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1077 - accuracy: 0.9760 - val_loss: 0.3835 - val_accuracy: 0.9469\n",
      "Epoch 15/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0975 - accuracy: 0.9782 - val_loss: 0.3889 - val_accuracy: 0.9443\n",
      "Epoch 16/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0959 - accuracy: 0.9795 - val_loss: 0.3743 - val_accuracy: 0.9432\n",
      "Epoch 17/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0900 - accuracy: 0.9793 - val_loss: 0.4203 - val_accuracy: 0.9464\n",
      "Epoch 18/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0972 - accuracy: 0.9783 - val_loss: 0.3984 - val_accuracy: 0.9443\n",
      "Epoch 19/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0937 - accuracy: 0.9795 - val_loss: 0.4631 - val_accuracy: 0.9443\n",
      "Epoch 20/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0992 - accuracy: 0.9796 - val_loss: 0.3848 - val_accuracy: 0.9454\n",
      "Epoch 21/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0900 - accuracy: 0.9815 - val_loss: 0.4191 - val_accuracy: 0.9448\n",
      "Epoch 22/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1012 - accuracy: 0.9809 - val_loss: 0.3769 - val_accuracy: 0.9480\n",
      "Epoch 23/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1035 - accuracy: 0.9804 - val_loss: 0.4220 - val_accuracy: 0.9475\n",
      "Epoch 24/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1027 - accuracy: 0.9812 - val_loss: 0.4371 - val_accuracy: 0.9454\n",
      "Epoch 25/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1024 - accuracy: 0.9798 - val_loss: 0.4519 - val_accuracy: 0.9464\n",
      "Epoch 26/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1171 - accuracy: 0.9792 - val_loss: 0.4254 - val_accuracy: 0.9422\n",
      "Epoch 27/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1064 - accuracy: 0.9790 - val_loss: 0.4255 - val_accuracy: 0.9459\n",
      "Epoch 28/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1024 - accuracy: 0.9811 - val_loss: 0.4218 - val_accuracy: 0.9485\n",
      "Epoch 29/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0955 - accuracy: 0.9814 - val_loss: 0.4308 - val_accuracy: 0.9485\n",
      "Epoch 30/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0995 - accuracy: 0.9827 - val_loss: 0.4235 - val_accuracy: 0.9454\n",
      "Epoch 31/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0905 - accuracy: 0.9826 - val_loss: 0.4881 - val_accuracy: 0.9491\n",
      "Epoch 32/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0907 - accuracy: 0.9835 - val_loss: 0.4484 - val_accuracy: 0.9480\n",
      "Epoch 33/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0935 - accuracy: 0.9826 - val_loss: 0.5006 - val_accuracy: 0.9459\n",
      "Epoch 34/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1039 - accuracy: 0.9822 - val_loss: 0.4707 - val_accuracy: 0.9459\n",
      "Epoch 35/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1010 - accuracy: 0.9833 - val_loss: 0.4631 - val_accuracy: 0.9469\n",
      "Epoch 36/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1042 - accuracy: 0.9826 - val_loss: 0.4376 - val_accuracy: 0.9475\n",
      "Epoch 37/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1064 - accuracy: 0.9819 - val_loss: 0.5288 - val_accuracy: 0.9448\n",
      "Epoch 38/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0939 - accuracy: 0.9840 - val_loss: 0.4192 - val_accuracy: 0.9438\n",
      "Epoch 39/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.1060 - accuracy: 0.9830 - val_loss: 0.4936 - val_accuracy: 0.9485\n",
      "Epoch 40/1000\n",
      "3389/3389 [==============================] - 5s 2ms/step - loss: 0.0906 - accuracy: 0.9855 - val_loss: 0.4653 - val_accuracy: 0.9475\n",
      "Epoch 41/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0846 - accuracy: 0.9857 - val_loss: 0.4598 - val_accuracy: 0.9480\n",
      "Epoch 42/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0869 - accuracy: 0.9851 - val_loss: 0.5002 - val_accuracy: 0.9469\n",
      "Epoch 43/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0929 - accuracy: 0.9858 - val_loss: 0.4802 - val_accuracy: 0.9459\n",
      "Epoch 44/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0873 - accuracy: 0.9864 - val_loss: 0.4394 - val_accuracy: 0.9480\n",
      "Epoch 45/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0871 - accuracy: 0.9867 - val_loss: 0.4554 - val_accuracy: 0.9507\n",
      "Epoch 46/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0949 - accuracy: 0.9867 - val_loss: 0.4700 - val_accuracy: 0.9480\n",
      "Epoch 47/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0962 - accuracy: 0.9865 - val_loss: 0.4402 - val_accuracy: 0.9475\n",
      "Epoch 48/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0737 - accuracy: 0.9874 - val_loss: 0.4740 - val_accuracy: 0.9501\n",
      "Epoch 49/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0907 - accuracy: 0.9864 - val_loss: 0.4557 - val_accuracy: 0.9512\n",
      "Epoch 50/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1070 - accuracy: 0.9850 - val_loss: 0.4883 - val_accuracy: 0.9422\n",
      "Epoch 51/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0943 - accuracy: 0.9862 - val_loss: 0.5106 - val_accuracy: 0.9480\n",
      "Epoch 52/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1037 - accuracy: 0.9852 - val_loss: 0.5118 - val_accuracy: 0.9512\n",
      "Epoch 53/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0941 - accuracy: 0.9866 - val_loss: 0.4921 - val_accuracy: 0.9501\n",
      "Epoch 54/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0858 - accuracy: 0.9886 - val_loss: 0.4941 - val_accuracy: 0.9512\n",
      "Epoch 55/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0821 - accuracy: 0.9883 - val_loss: 0.4664 - val_accuracy: 0.9538\n",
      "Epoch 56/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0851 - accuracy: 0.9883 - val_loss: 0.5632 - val_accuracy: 0.9507\n",
      "Epoch 57/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0919 - accuracy: 0.9877 - val_loss: 0.5142 - val_accuracy: 0.9480\n",
      "Epoch 58/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0820 - accuracy: 0.9876 - val_loss: 0.5233 - val_accuracy: 0.9480\n",
      "Epoch 59/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1043 - accuracy: 0.9864 - val_loss: 0.5550 - val_accuracy: 0.9448\n",
      "Epoch 60/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0888 - accuracy: 0.9877 - val_loss: 0.5670 - val_accuracy: 0.9475\n",
      "Epoch 61/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0851 - accuracy: 0.9888 - val_loss: 0.5903 - val_accuracy: 0.9422\n",
      "Epoch 62/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0836 - accuracy: 0.9894 - val_loss: 0.5881 - val_accuracy: 0.9464\n",
      "Epoch 63/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0739 - accuracy: 0.9887 - val_loss: 0.5526 - val_accuracy: 0.9459\n",
      "Epoch 64/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0704 - accuracy: 0.9913 - val_loss: 0.6182 - val_accuracy: 0.9459\n",
      "Epoch 65/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0689 - accuracy: 0.9906 - val_loss: 0.5602 - val_accuracy: 0.9491\n",
      "Epoch 66/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0843 - accuracy: 0.9891 - val_loss: 0.6003 - val_accuracy: 0.9469\n",
      "Epoch 67/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0744 - accuracy: 0.9900 - val_loss: 0.6024 - val_accuracy: 0.9459\n",
      "Epoch 68/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0675 - accuracy: 0.9906 - val_loss: 0.5746 - val_accuracy: 0.9454\n",
      "Epoch 69/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0626 - accuracy: 0.9907 - val_loss: 0.5807 - val_accuracy: 0.9464\n",
      "Epoch 70/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0672 - accuracy: 0.9900 - val_loss: 0.5797 - val_accuracy: 0.9464\n",
      "Epoch 71/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0845 - accuracy: 0.9894 - val_loss: 0.5800 - val_accuracy: 0.9491\n",
      "Epoch 72/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0688 - accuracy: 0.9916 - val_loss: 0.6037 - val_accuracy: 0.9480\n",
      "Epoch 73/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0850 - accuracy: 0.9894 - val_loss: 0.5870 - val_accuracy: 0.9459\n",
      "Epoch 74/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0730 - accuracy: 0.9913 - val_loss: 0.5851 - val_accuracy: 0.9480\n",
      "Epoch 75/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0700 - accuracy: 0.9917 - val_loss: 0.5835 - val_accuracy: 0.9491\n",
      "Epoch 76/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0665 - accuracy: 0.9917 - val_loss: 0.6310 - val_accuracy: 0.9485\n",
      "Epoch 77/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0665 - accuracy: 0.9917 - val_loss: 0.5922 - val_accuracy: 0.9475\n",
      "Epoch 78/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0644 - accuracy: 0.9922 - val_loss: 0.5873 - val_accuracy: 0.9469\n",
      "Epoch 79/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0663 - accuracy: 0.9913 - val_loss: 0.5090 - val_accuracy: 0.9501\n",
      "Epoch 80/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0728 - accuracy: 0.9910 - val_loss: 0.5914 - val_accuracy: 0.9507\n",
      "Epoch 81/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0773 - accuracy: 0.9916 - val_loss: 0.5649 - val_accuracy: 0.9512\n",
      "Epoch 82/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0813 - accuracy: 0.9915 - val_loss: 0.5608 - val_accuracy: 0.9491\n",
      "Epoch 83/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0596 - accuracy: 0.9932 - val_loss: 0.5417 - val_accuracy: 0.9501\n",
      "Epoch 84/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0721 - accuracy: 0.9917 - val_loss: 0.5562 - val_accuracy: 0.9517\n",
      "Epoch 85/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0776 - accuracy: 0.9916 - val_loss: 0.5391 - val_accuracy: 0.9523\n",
      "Epoch 86/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0861 - accuracy: 0.9904 - val_loss: 0.5298 - val_accuracy: 0.9528\n",
      "Epoch 87/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0830 - accuracy: 0.9907 - val_loss: 0.6099 - val_accuracy: 0.9507\n",
      "Epoch 88/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0836 - accuracy: 0.9910 - val_loss: 0.5656 - val_accuracy: 0.9512\n",
      "Epoch 89/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0789 - accuracy: 0.9908 - val_loss: 0.5726 - val_accuracy: 0.9507\n",
      "Epoch 90/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0822 - accuracy: 0.9911 - val_loss: 0.5454 - val_accuracy: 0.9507\n",
      "Epoch 91/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0798 - accuracy: 0.9917 - val_loss: 0.5515 - val_accuracy: 0.9501\n",
      "Epoch 92/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0755 - accuracy: 0.9916 - val_loss: 0.6122 - val_accuracy: 0.9496\n",
      "Epoch 93/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0918 - accuracy: 0.9901 - val_loss: 0.6298 - val_accuracy: 0.9512\n",
      "Epoch 94/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0830 - accuracy: 0.9911 - val_loss: 0.6186 - val_accuracy: 0.9512\n",
      "Epoch 95/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0612 - accuracy: 0.9930 - val_loss: 0.6188 - val_accuracy: 0.9501\n",
      "Epoch 96/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0803 - accuracy: 0.9913 - val_loss: 0.6265 - val_accuracy: 0.9496\n",
      "Epoch 97/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0742 - accuracy: 0.9925 - val_loss: 0.6421 - val_accuracy: 0.9507\n",
      "Epoch 98/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0806 - accuracy: 0.9911 - val_loss: 0.6311 - val_accuracy: 0.9491\n",
      "Epoch 99/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0769 - accuracy: 0.9911 - val_loss: 0.6158 - val_accuracy: 0.9491\n",
      "Epoch 100/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0743 - accuracy: 0.9914 - val_loss: 0.5990 - val_accuracy: 0.9523\n",
      "Epoch 101/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0742 - accuracy: 0.9917 - val_loss: 0.6069 - val_accuracy: 0.9512\n",
      "Epoch 102/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0797 - accuracy: 0.9913 - val_loss: 0.6263 - val_accuracy: 0.9501\n",
      "Epoch 103/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0801 - accuracy: 0.9920 - val_loss: 0.6790 - val_accuracy: 0.9507\n",
      "Epoch 104/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0797 - accuracy: 0.9917 - val_loss: 0.6118 - val_accuracy: 0.9501\n",
      "Epoch 105/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0784 - accuracy: 0.9920 - val_loss: 0.6181 - val_accuracy: 0.9517\n",
      "Epoch 106/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0807 - accuracy: 0.9920 - val_loss: 0.6352 - val_accuracy: 0.9507\n",
      "Epoch 107/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0736 - accuracy: 0.9923 - val_loss: 0.6549 - val_accuracy: 0.9485\n",
      "Epoch 108/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0699 - accuracy: 0.9928 - val_loss: 0.6606 - val_accuracy: 0.9507\n",
      "Epoch 109/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0760 - accuracy: 0.9919 - val_loss: 0.6391 - val_accuracy: 0.9491\n",
      "Epoch 110/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0792 - accuracy: 0.9920 - val_loss: 0.6144 - val_accuracy: 0.9496\n",
      "Epoch 111/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0913 - accuracy: 0.9910 - val_loss: 0.6089 - val_accuracy: 0.9496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0786 - accuracy: 0.9923 - val_loss: 0.6151 - val_accuracy: 0.9496\n",
      "Epoch 113/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0653 - accuracy: 0.9931 - val_loss: 0.5904 - val_accuracy: 0.9507\n",
      "Epoch 114/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0691 - accuracy: 0.9926 - val_loss: 0.6773 - val_accuracy: 0.9475\n",
      "Epoch 115/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0675 - accuracy: 0.9932 - val_loss: 0.6742 - val_accuracy: 0.9507\n",
      "Epoch 116/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0779 - accuracy: 0.9923 - val_loss: 0.6135 - val_accuracy: 0.9501\n",
      "Epoch 117/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0686 - accuracy: 0.9928 - val_loss: 0.6375 - val_accuracy: 0.9485\n",
      "Epoch 118/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0660 - accuracy: 0.9933 - val_loss: 0.6553 - val_accuracy: 0.9480\n",
      "Epoch 119/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0622 - accuracy: 0.9933 - val_loss: 0.7014 - val_accuracy: 0.9464\n",
      "Epoch 120/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0782 - accuracy: 0.9923 - val_loss: 0.6912 - val_accuracy: 0.9480\n",
      "Epoch 121/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0750 - accuracy: 0.9925 - val_loss: 0.6966 - val_accuracy: 0.9485\n",
      "Epoch 122/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0775 - accuracy: 0.9924 - val_loss: 0.7142 - val_accuracy: 0.9475\n",
      "Epoch 123/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0767 - accuracy: 0.9923 - val_loss: 0.6635 - val_accuracy: 0.9485\n",
      "Epoch 124/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0821 - accuracy: 0.9913 - val_loss: 0.6033 - val_accuracy: 0.9512\n",
      "Epoch 125/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0735 - accuracy: 0.9929 - val_loss: 0.6777 - val_accuracy: 0.9496\n",
      "Epoch 126/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0829 - accuracy: 0.9917 - val_loss: 0.6358 - val_accuracy: 0.9491\n",
      "Epoch 127/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0985 - accuracy: 0.9906 - val_loss: 0.6621 - val_accuracy: 0.9491\n",
      "Epoch 128/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1025 - accuracy: 0.9906 - val_loss: 0.6643 - val_accuracy: 0.9496\n",
      "Epoch 129/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0950 - accuracy: 0.9907 - val_loss: 0.6700 - val_accuracy: 0.9480\n",
      "Epoch 130/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0953 - accuracy: 0.9909 - val_loss: 0.6805 - val_accuracy: 0.9480\n",
      "Epoch 131/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1064 - accuracy: 0.9903 - val_loss: 0.6907 - val_accuracy: 0.9491\n",
      "Epoch 132/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0956 - accuracy: 0.9911 - val_loss: 0.7045 - val_accuracy: 0.9469\n",
      "Epoch 133/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1030 - accuracy: 0.9909 - val_loss: 0.6745 - val_accuracy: 0.9485\n",
      "Epoch 134/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0934 - accuracy: 0.9920 - val_loss: 0.6454 - val_accuracy: 0.9496\n",
      "Epoch 135/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0955 - accuracy: 0.9916 - val_loss: 0.7091 - val_accuracy: 0.9459\n",
      "Epoch 136/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0957 - accuracy: 0.9916 - val_loss: 0.7396 - val_accuracy: 0.9459\n",
      "Epoch 137/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1041 - accuracy: 0.9911 - val_loss: 0.7265 - val_accuracy: 0.9469\n",
      "Epoch 138/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1060 - accuracy: 0.9905 - val_loss: 0.7247 - val_accuracy: 0.9459\n",
      "Epoch 139/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0922 - accuracy: 0.9911 - val_loss: 0.7186 - val_accuracy: 0.9448\n",
      "Epoch 140/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0963 - accuracy: 0.9909 - val_loss: 0.7040 - val_accuracy: 0.9459\n",
      "Epoch 141/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0899 - accuracy: 0.9916 - val_loss: 0.6897 - val_accuracy: 0.9459\n",
      "Epoch 142/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0779 - accuracy: 0.9928 - val_loss: 0.6966 - val_accuracy: 0.9475\n",
      "Epoch 143/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0768 - accuracy: 0.9926 - val_loss: 0.7167 - val_accuracy: 0.9464\n",
      "Epoch 144/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0844 - accuracy: 0.9923 - val_loss: 0.7444 - val_accuracy: 0.9459\n",
      "Epoch 145/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0931 - accuracy: 0.9919 - val_loss: 0.7185 - val_accuracy: 0.9475\n",
      "Epoch 146/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0970 - accuracy: 0.9913 - val_loss: 0.6589 - val_accuracy: 0.9469\n",
      "Epoch 147/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1055 - accuracy: 0.9906 - val_loss: 0.6859 - val_accuracy: 0.9469\n",
      "Epoch 148/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0890 - accuracy: 0.9918 - val_loss: 0.7209 - val_accuracy: 0.9464\n",
      "Epoch 149/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0871 - accuracy: 0.9918 - val_loss: 0.7629 - val_accuracy: 0.9459\n",
      "Epoch 150/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0813 - accuracy: 0.9923 - val_loss: 0.7674 - val_accuracy: 0.9469\n",
      "Epoch 151/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0806 - accuracy: 0.9927 - val_loss: 0.7254 - val_accuracy: 0.9464\n",
      "Epoch 152/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0948 - accuracy: 0.9914 - val_loss: 0.6866 - val_accuracy: 0.9496\n",
      "Epoch 153/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.1028 - accuracy: 0.9903 - val_loss: 0.7109 - val_accuracy: 0.9454\n",
      "Epoch 154/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0871 - accuracy: 0.9919 - val_loss: 0.7319 - val_accuracy: 0.9454\n",
      "Epoch 155/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0883 - accuracy: 0.9924 - val_loss: 0.7424 - val_accuracy: 0.9464\n",
      "Epoch 156/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0807 - accuracy: 0.9930 - val_loss: 0.7469 - val_accuracy: 0.9469\n",
      "Epoch 157/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0868 - accuracy: 0.9924 - val_loss: 0.7459 - val_accuracy: 0.9469\n",
      "Epoch 158/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0792 - accuracy: 0.9927 - val_loss: 0.7407 - val_accuracy: 0.9480\n",
      "Epoch 159/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0810 - accuracy: 0.9927 - val_loss: 0.7280 - val_accuracy: 0.9464\n",
      "Epoch 160/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0777 - accuracy: 0.9930 - val_loss: 0.7196 - val_accuracy: 0.9469\n",
      "Epoch 161/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0720 - accuracy: 0.9931 - val_loss: 0.7404 - val_accuracy: 0.9485\n",
      "Epoch 162/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0697 - accuracy: 0.9943 - val_loss: 0.7093 - val_accuracy: 0.9491\n",
      "Epoch 163/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0794 - accuracy: 0.9937 - val_loss: 0.7299 - val_accuracy: 0.9480\n",
      "Epoch 164/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0731 - accuracy: 0.9935 - val_loss: 0.7163 - val_accuracy: 0.9469\n",
      "Epoch 165/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0724 - accuracy: 0.9934 - val_loss: 0.7482 - val_accuracy: 0.9480\n",
      "Epoch 166/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0690 - accuracy: 0.9934 - val_loss: 0.7628 - val_accuracy: 0.9480\n",
      "Epoch 167/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0793 - accuracy: 0.9932 - val_loss: 0.7587 - val_accuracy: 0.9475\n",
      "Epoch 168/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0801 - accuracy: 0.9928 - val_loss: 0.7759 - val_accuracy: 0.9475\n",
      "Epoch 169/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0874 - accuracy: 0.9922 - val_loss: 0.7476 - val_accuracy: 0.9475\n",
      "Epoch 170/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0811 - accuracy: 0.9929 - val_loss: 0.7543 - val_accuracy: 0.9469\n",
      "Epoch 171/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0886 - accuracy: 0.9919 - val_loss: 0.7479 - val_accuracy: 0.9485\n",
      "Epoch 172/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0801 - accuracy: 0.9930 - val_loss: 0.7335 - val_accuracy: 0.9491\n",
      "Epoch 173/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0773 - accuracy: 0.9932 - val_loss: 0.7293 - val_accuracy: 0.9491\n",
      "Epoch 174/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0914 - accuracy: 0.9922 - val_loss: 0.7238 - val_accuracy: 0.9485\n",
      "Epoch 175/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0873 - accuracy: 0.9923 - val_loss: 0.7779 - val_accuracy: 0.9459\n",
      "Epoch 176/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0711 - accuracy: 0.9937 - val_loss: 0.7986 - val_accuracy: 0.9454\n",
      "Epoch 177/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0822 - accuracy: 0.9926 - val_loss: 0.7709 - val_accuracy: 0.9475\n",
      "Epoch 178/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0784 - accuracy: 0.9936 - val_loss: 0.7311 - val_accuracy: 0.9480\n",
      "Epoch 179/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0789 - accuracy: 0.9933 - val_loss: 0.7464 - val_accuracy: 0.9491\n",
      "Epoch 180/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0759 - accuracy: 0.9934 - val_loss: 0.7468 - val_accuracy: 0.9491\n",
      "Epoch 181/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0783 - accuracy: 0.9934 - val_loss: 0.7438 - val_accuracy: 0.9496\n",
      "Epoch 182/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0710 - accuracy: 0.9936 - val_loss: 0.7467 - val_accuracy: 0.9485\n",
      "Epoch 183/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0726 - accuracy: 0.9936 - val_loss: 0.7358 - val_accuracy: 0.9485\n",
      "Epoch 184/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0805 - accuracy: 0.9935 - val_loss: 0.7467 - val_accuracy: 0.9496\n",
      "Epoch 185/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0838 - accuracy: 0.9930 - val_loss: 0.7657 - val_accuracy: 0.9454\n",
      "Epoch 186/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0845 - accuracy: 0.9926 - val_loss: 0.7621 - val_accuracy: 0.9464\n",
      "Epoch 187/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0825 - accuracy: 0.9930 - val_loss: 0.7803 - val_accuracy: 0.9469\n",
      "Epoch 188/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0930 - accuracy: 0.9923 - val_loss: 0.7762 - val_accuracy: 0.9469\n",
      "Epoch 189/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0707 - accuracy: 0.9939 - val_loss: 0.7644 - val_accuracy: 0.9475\n",
      "Epoch 190/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0718 - accuracy: 0.9934 - val_loss: 0.7527 - val_accuracy: 0.9480\n",
      "Epoch 191/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0793 - accuracy: 0.9933 - val_loss: 0.7684 - val_accuracy: 0.9454\n",
      "Epoch 192/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0705 - accuracy: 0.9940 - val_loss: 0.7291 - val_accuracy: 0.9480\n",
      "Epoch 193/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0688 - accuracy: 0.9935 - val_loss: 0.7507 - val_accuracy: 0.9464\n",
      "Epoch 194/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0742 - accuracy: 0.9933 - val_loss: 0.7383 - val_accuracy: 0.9491\n",
      "Epoch 195/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0742 - accuracy: 0.9941 - val_loss: 0.7535 - val_accuracy: 0.9480\n",
      "Epoch 196/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0709 - accuracy: 0.9939 - val_loss: 0.7409 - val_accuracy: 0.9475\n",
      "Epoch 197/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0830 - accuracy: 0.9929 - val_loss: 0.7424 - val_accuracy: 0.9485\n",
      "Epoch 198/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0832 - accuracy: 0.9932 - val_loss: 0.7660 - val_accuracy: 0.9480\n",
      "Epoch 199/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0872 - accuracy: 0.9926 - val_loss: 0.7520 - val_accuracy: 0.9491\n",
      "Epoch 200/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0841 - accuracy: 0.9927 - val_loss: 0.7786 - val_accuracy: 0.9475\n",
      "Epoch 201/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.1046 - accuracy: 0.9911 - val_loss: 0.7868 - val_accuracy: 0.9480\n",
      "Epoch 202/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0905 - accuracy: 0.9923 - val_loss: 0.8037 - val_accuracy: 0.9475\n",
      "Epoch 203/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0930 - accuracy: 0.9922 - val_loss: 0.7608 - val_accuracy: 0.9475\n",
      "Epoch 204/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0857 - accuracy: 0.9927 - val_loss: 0.7703 - val_accuracy: 0.9480\n",
      "Epoch 205/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0780 - accuracy: 0.9936 - val_loss: 0.7540 - val_accuracy: 0.9480\n",
      "Epoch 206/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0809 - accuracy: 0.9930 - val_loss: 0.7686 - val_accuracy: 0.9491\n",
      "Epoch 207/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0921 - accuracy: 0.9923 - val_loss: 0.7725 - val_accuracy: 0.9485\n",
      "Epoch 208/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0900 - accuracy: 0.9926 - val_loss: 0.7765 - val_accuracy: 0.9496\n",
      "Epoch 209/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0863 - accuracy: 0.9927 - val_loss: 0.7798 - val_accuracy: 0.9496\n",
      "Epoch 210/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0843 - accuracy: 0.9934 - val_loss: 0.7670 - val_accuracy: 0.9501\n",
      "Epoch 211/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0931 - accuracy: 0.9924 - val_loss: 0.7648 - val_accuracy: 0.9485\n",
      "Epoch 212/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0813 - accuracy: 0.9933 - val_loss: 0.7648 - val_accuracy: 0.9496\n",
      "Epoch 213/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0986 - accuracy: 0.9919 - val_loss: 0.7213 - val_accuracy: 0.9491\n",
      "Epoch 214/1000\n",
      "3389/3389 [==============================] - 4s 1ms/step - loss: 0.0946 - accuracy: 0.9923 - val_loss: 0.7447 - val_accuracy: 0.9491\n",
      "Epoch 215/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0945 - accuracy: 0.9923 - val_loss: 0.7553 - val_accuracy: 0.9491\n",
      "Epoch 216/1000\n",
      "3389/3389 [==============================] - 5s 1ms/step - loss: 0.0719 - accuracy: 0.9935 - val_loss: 0.7666 - val_accuracy: 0.9475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x141eeac10>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# filename = 'cheez_translate_005.h5'\n",
    "# checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=200),\n",
    "            ModelCheckpoint(filepath='1000_epochs_100_batch_patience_200_spell_check_12_10_19.h5', \n",
    "                            monitor='val_loss', \n",
    "                            save_best_only=True),\n",
    "            TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "            ]\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, \n",
    "                    preproc_clean, \n",
    "                    batch_size=100, \n",
    "                    epochs=1000, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.monger/anaconda3/envs/name_quality/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "embed_rnn_model = load_model('abbrev_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4166), started 2:59:34 ago. (Use '!kill 4166' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e52ae9267a44b51\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e52ae9267a44b51\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tensorboard/r2/tensorboard_in_notebooks\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use abbrev_tokenizer here and clean tokenizer below\n",
    "result = np.zeros((1,5))\n",
    "result\n",
    "\n",
    "da = abbrev_tokenizer.texts_to_sequences([\"Boat GRN ONIN\"])\n",
    "# !!! \"GRN ONIN\" is not in the label file, so it is generalizising at the character level\n",
    "da = pad(np.array([da[0][:5]]))\n",
    "# da = da.astype(int)\n",
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 2004, 1163]], dtype=int32)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UKN> <UKN> <UKN>']"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokenizer.sequences_to_texts(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:da.shape[0],:da.shape[1]] = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 2.004e+03, 1.163e+03, 0.000e+00, 0.000e+00]])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all preds\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37835822, 1.        , 0.9999918 , 1.        , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  https://www.kaggle.com/hamishdickson/using-keras-oov-tokens\n",
    "# <unk> Can just use it as a repository for words we dont know and may want to add\n",
    "\n",
    "preds = np.max(embed_rnn_model.predict(result)[0], 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_index = np.argmax(embed_rnn_model.predict(result)[0],1)\n",
    "preds_index\n",
    "\n",
    "preds = np.max(embed_rnn_model.predict(result)[0], 1)\n",
    "preds\n",
    "\n",
    "prob_threshold = .80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UKN> green onion <PAD> <PAD>'"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_to_text(preds_index,preds,prob_threshold, clean_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abbrev_tokenizer.word_index\n",
    "# clean_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UKN>': 1, 'the': 2, 'jumps': 3, 'over': 4, 'lazy': 5, 'dog': 6, 'quick': 7, 'brown': 8, 'fox': 9, 'a': 10, 'frog': 11}\n",
      "[[2, 6, 3, 4, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/keras-team/keras/issues/9574\n",
    "    \n",
    "num_words=10\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<UKN>\")\n",
    "tokenizer.fit_on_texts([\"The quick brown fox jumps over the lazy dog. The lazy dog jumps over a frog.\"])\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.texts_to_sequences([\"the dog jumps over the elephant\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Currie32/Spell-Checker/blob/master/SpellChecker.ipynb\n",
    "\n",
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    \n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0,1,1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0,1,1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i+1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(vocab_to_int[random_letter])\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass     \n",
    "        i += 1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sorted = [\"the cow ate grass\",\"the dog barked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check to ensure noise_maker is making mistakes correctly.\n",
    "threshold = 0.9\n",
    "for sentence in training_sorted[:5]:\n",
    "    print(sentence)\n",
    "    print(noise_maker(sentence, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_csv('bi_rnn_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules: \n",
    "#     -- Give it the words around for context. don't have to be next to eachother in product name\n",
    "#     -- Get rid of extra random stuff\n",
    "#     -- seperate words that are together 'HERSHEYSCHOCOLATE'\n",
    "#     --can often just copy and paste bad text into google and it will fix it (using quality model on ~0.02 items)\n",
    "#         --or could use google API: https://developers.google.com/custom-search/v1/overview?authuser=1\n",
    "#         --custom search API key: AIzaSyDEvYVrdsqqfe6XwxLxEgSI3ph2sMfLMrc \n",
    "#                 --https://github.com/googleapis/google-api-python-client\n",
    "#     --don't need to do 'chkn' a bunch.  Need a balanced dataset.  Can do undersampling/oversampling also\n",
    "    \n",
    "# try active learning next...\n",
    "# https://hackernoon.com/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
